[00:00:00] [music]
[00:00:06] [music] I'm still
[00:00:08] ass.
[00:00:12] [music]
[00:00:17] [music]
[00:00:24] All right, guys. Let's get started.
[00:00:26] Round of applause for DD Cash. Awesome.
[00:00:28] [applause]
[00:00:28] Again,
[00:00:31] your bank account is is is taken care of
[00:00:33] now or just to wait for money to come
[00:00:34] in.
[00:00:35] >> It's getting better. It's getting
[00:00:35] better.
[00:00:36] >> Okay. Yeah. Again, I got to take it by
[00:00:38] DJ. I can make sure he's doing okay. All
[00:00:39] right, guys. Lot to cover. Um because
[00:00:42] there's a bunch of stuff we we didn't we
[00:00:43] didn't cover last class. Uh at the end,
[00:00:45] we rushed through. I want to pick up on
[00:00:47] on that. So, again, midterm midterm
[00:00:49] exams. You want to come to my office
[00:00:50] hours after class today, you can come
[00:00:52] look at them with the solution. Homework
[00:00:54] four is due this Sunday. And then
[00:00:55] project three, I forgot this recitation
[00:00:58] was last night. I'll post that on Piaza
[00:01:00] after class with the video and the
[00:01:02] slides. Okay, any questions about
[00:01:03] homework four or project three?
[00:01:07] All right, so last class we were talking
[00:01:10] about the the query optimizer, right?
[00:01:12] And we sort of kicked off the discussion
[00:01:14] talking about how the query optimizer is
[00:01:16] the component of the system that is
[00:01:18] responsible for taking a SQL query and
[00:01:20] then generating a physical plan
[00:01:22] execution plan that the data system can
[00:01:24] then actually execute. And we talked
[00:01:26] about how we went from the the the the
[00:01:29] SQL we parse it generate an apps index
[00:01:31] tree. We run it through the binder where
[00:01:33] where we resolve names of tables of
[00:01:35] column names and functions to actual
[00:01:37] internal identifiers. And then with that
[00:01:39] logical plan, we can do uh some
[00:01:41] additional transformations to uh without
[00:01:45] considering a cost model to try to fix
[00:01:46] things up and make a little bit better
[00:01:48] than sort of a literal translation of
[00:01:50] the the the SQL query from the abstract
[00:01:52] syntax tree. Um and then we talked a
[00:01:54] little at the end, but we ran out of
[00:01:55] time and we'll pick up where we left
[00:01:56] off. Uh talking about the costbased
[00:02:00] search where we try to look at
[00:02:02] alternative plans, alternative physical
[00:02:04] plans and pick the one that we think is
[00:02:06] going to have the lowest cost. And
[00:02:07] again, I'm putting cost here in in
[00:02:10] parenthesis because the cost is going to
[00:02:11] vary per per system, per environment,
[00:02:14] what you're trying to optimize for. But
[00:02:16] usually, you're going to try to optimize
[00:02:18] the amount of dis IO's in in a very
[00:02:20] basic cost model. Uh, but you can build
[00:02:22] upon that and do more sophisticated
[00:02:24] things like considering how much it
[00:02:25] takes to to compute a function or uh to
[00:02:29] process something that's in memory. And
[00:02:31] then we said the the the the way we
[00:02:33] determine or way we implement our our
[00:02:36] sort of costbased query optimizer the
[00:02:39] quality of the plans it's going to
[00:02:40] produce is going to be a combination of
[00:02:42] these three factors. So what kind of
[00:02:44] transformation rules do we have that
[00:02:46] will convert the logical plan to another
[00:02:48] logical plan or logical plan to a
[00:02:49] physical plan and this allows us to
[00:02:51] enumerate over different permutations of
[00:02:53] the query uh the query plan and try to
[00:02:55] find one with a you know with a better
[00:02:57] cost. Um then there'll be the search
[00:02:59] algorithm which we we rushed at the end.
[00:03:01] We'll pick up again now. But this is the
[00:03:03] the method in which we're going to
[00:03:05] explore the different possibilities of
[00:03:07] alternative plans we have being
[00:03:09] generated by the transformation rules.
[00:03:11] So the numeration process then search
[00:03:13] for them and try to find better better
[00:03:15] choices. And then the cost model piece
[00:03:17] that we'll talk about at the end that's
[00:03:18] where we try to predict what will be the
[00:03:20] expected runtime behavior of a query
[00:03:23] plan. Not so much to trying to to
[00:03:26] generate an absolute value like this
[00:03:28] query is going to take 15 seconds or you
[00:03:30] know you're not trying to pick something
[00:03:31] that's going to be like something you
[00:03:33] can measure in the real world. What
[00:03:35] you're really trying to do with this
[00:03:36] cost model is to identify whether one
[00:03:38] query plan is better than another. So
[00:03:40] this cost model will be an internal
[00:03:42] metric we use that is in you know that
[00:03:45] that the database optimizer can only
[00:03:47] use. So whatever cost model that my SQL
[00:03:49] is using they spit out a number say this
[00:03:51] is the cost of a query plan that's
[00:03:52] meaningless in Postgress or Oracle or
[00:03:54] pick your favorite database system. So
[00:03:56] this the cost model is the internal me
[00:03:58] mechanism we use to figure this out. So
[00:04:00] for today's class we'll we'll come back
[00:04:02] to the search algorithms the top top to
[00:04:04] the top to the bottom versus bottom to
[00:04:06] the top. uh and then we'll talk about
[00:04:08] the kind of statistics we would collect
[00:04:09] and then the cost models we use to
[00:04:12] estimate the cardality of the operators
[00:04:14] because that's going to be the big thing
[00:04:15] what we have to determine how much work
[00:04:16] a query is going to uh consume or or or
[00:04:19] use because that's going to tell us how
[00:04:21] much data we're sending between the
[00:04:22] different operators. Okay. And again
[00:04:25] just to preface this this is super hard.
[00:04:27] This is the hardest part of of of
[00:04:29] database systems. I I this is the part I
[00:04:32] know the least about. Uh so you know
[00:04:36] don't feel like if you're not following
[00:04:37] this along this is super challenging or
[00:04:39] we're barely scratching the surface
[00:04:40] here. Uh
[00:04:43] and like long as you understand the high
[00:04:44] level concepts that's that's what really
[00:04:46] matters and then the details of this
[00:04:48] maybe only matters if you actually go
[00:04:49] end up building a query optimizer.
[00:04:53] All right. Right. So again, the search
[00:04:54] alg is is the way we're going to again
[00:04:57] enumerate or look at different
[00:04:59] alternative plans or choices we have for
[00:05:02] both logical operators and and and their
[00:05:04] corresponding physical operators in our
[00:05:06] our query plan. And the two the two
[00:05:09] approaches either start from the bottom
[00:05:10] and work your way to the top sometimes
[00:05:12] called forward chaining. Um and then the
[00:05:16] the the alternate approach is start from
[00:05:18] the top and then you work your way to
[00:05:19] the bottom to to the leaf nodes. And
[00:05:21] again, conceptually they're sort of the
[00:05:23] same at a high level, right? The thing
[00:05:27] they're trying to produce is the same.
[00:05:28] They're trying to generate a physical
[00:05:29] plan that I can then go execute, but
[00:05:31] there's some nuances to uh to the
[00:05:33] different approaches in terms of like
[00:05:35] how much um
[00:05:38] uh like how much backtracking or how
[00:05:40] much redundant work you would have to
[00:05:41] use you could possibly
[00:05:44] uh encounter while you're generating
[00:05:46] these query plans. And in particular,
[00:05:48] the bottom one, they're going to make
[00:05:49] heavy use of memorization to avoid, you
[00:05:52] know, redundant superolous computations.
[00:05:54] All right, [snorts] so bottom up
[00:05:56] basically is you start with the leaf
[00:05:58] nodes in your query plan. You basically
[00:05:59] start with nothing like here's how I'm
[00:06:01] going to access the individual tables
[00:06:03] and then I'm going to figure out now
[00:06:05] what order I want to generate the joins
[00:06:07] and other parts of the query plan. Uh,
[00:06:09] working the way to the top where you end
[00:06:11] up with the the final result that you
[00:06:13] want. Like I want to join these tables
[00:06:14] and produce this output. top down is is
[00:06:17] the opposite where you start with with
[00:06:19] with what you want to produce. Start
[00:06:20] with what the outcome you want to be for
[00:06:21] the query and then you work the way down
[00:06:24] and try to figure out what operators I
[00:06:26] need to introduce to get me back to to
[00:06:28] that location.
[00:06:30] So the very first query optimizer uh
[00:06:32] that IBM built the first sorry very very
[00:06:34] first costbased query optimizer that IBM
[00:06:37] built in the 1970s in system R follows
[00:06:39] this top approach here and I'll show
[00:06:42] this next slide. Most most open source
[00:06:44] data systems that are out today that you
[00:06:46] know about are doing something like
[00:06:47] this. Postgress is doing something that
[00:06:49] sort of looks like this. DB2 is doing
[00:06:51] this. The Germans do this but like on
[00:06:53] steroids. Uh and then duct DB follows a
[00:06:56] a some of the work that the Germans did
[00:06:59] at Munich 10 years ago. And it's it's
[00:07:02] following this approach as well. Right.
[00:07:04] [snorts] So again, the way this is going
[00:07:05] to work is that we're gonna we're use
[00:07:07] our transformation rules to do some
[00:07:08] initial optimizations to again do
[00:07:10] predicate push down, do the things we
[00:07:12] know we're always going to want to do to
[00:07:14] optimize our query without actually
[00:07:15] consulting a cost model. And then we're
[00:07:17] going to use this divide and conquer
[00:07:18] algorithm uh that's going to again sort
[00:07:21] of search at sort of different join
[00:07:24] levels uh in the tree that I'll show in
[00:07:26] this next slide like which going to be
[00:07:28] the optimal joint ordering and what
[00:07:30] physical algorithm I'm going to want to
[00:07:31] use to compute the the different joins.
[00:07:36] So let's say again we have we have a
[00:07:37] query like this. We're doing a three-way
[00:07:38] joint and artist appears an album. This
[00:07:40] is the example I showed at the beginning
[00:07:41] of the semester, right? And so in the
[00:07:44] system R implementation the first stage
[00:07:46] you you basically apply a bunch of
[00:07:48] transformation rules that try to figure
[00:07:50] out for each of the tables I need to
[00:07:52] access what's the best access method to
[00:07:54] use right and this one would be simple
[00:07:56] things like well I in the case of artist
[00:07:58] I don't have a I don't have anything I
[00:08:01] could use as a as a as a as a filter
[00:08:04] directly on my table because there's no
[00:08:06] like you know where artist name equals
[00:08:07] something right so therefore I'm I'm
[00:08:10] going to want to do a scential scan but
[00:08:12] in the case of of the album table. I
[00:08:13] have an index on the name attribute. So
[00:08:15] I can use that in a uh in my in my
[00:08:18] predicate.
[00:08:20] So then now the next stage and this is
[00:08:21] where the divide and conquer piece comes
[00:08:23] in. Uh where we're going to we're going
[00:08:25] to seed out all the different possible
[00:08:28] join earnings I have for the tables,
[00:08:30] right? Like do I want to join artist and
[00:08:32] appears an album first or appears album
[00:08:34] and artist first? Right? You just do
[00:08:36] this for all possible combinations
[00:08:37] including introducing cartisian
[00:08:39] products, right? because those those are
[00:08:41] still valid joins. And then now I'm
[00:08:43] going to run this uh dynamic programming
[00:08:45] algorithm, the divide and conquer
[00:08:46] approach where I'm going to again now
[00:08:49] start examining each of these different
[00:08:51] combinations I could have to get to my
[00:08:53] final outcome because I'm starting at
[00:08:54] the bottom and choose the one that's
[00:08:56] going to have the lowest cost.
[00:09:02] >> Say it again.
[00:09:06] >> Correct. Yes.
[00:09:09] that kind of in this kind of scenario
[00:09:12] maybe
[00:09:15] >> right so state David is and he's correct
[00:09:17] in the last class I said that the
[00:09:19] largest uh query I've seen with with
[00:09:21] joins in terms of number of tables was
[00:09:22] like 1500600 something um and then
[00:09:26] wouldn't this explode in terms of number
[00:09:28] of choices you have to consider
[00:09:29] absolutely yes
[00:09:34] >> before explaining with what sorry
[00:09:36] >> become so large that it stops like an
[00:09:38] early.
[00:09:40] >> Yeah.
[00:09:41] >> So he's correct like this is like
[00:09:44] [snorts]
[00:09:45] if I if I do this isn't going to be the
[00:09:46] search base is huge and therefore I'm
[00:09:48] never going to can even even try to get
[00:09:50] to something reasonable. So they're
[00:09:53] they'll do a bunch of things like throw
[00:09:54] away last class we talked about we throw
[00:09:56] away anything that's not a left deep
[00:09:57] join tree. So that throwways away a
[00:09:59] bunch bunch of possibilities I could
[00:10:00] have uh in terms of terms of these
[00:10:02] orderings. I would throw away any
[00:10:04] cartisian products right away. That's
[00:10:06] easy. There's other things you can do
[00:10:08] like if I know it's a uh comment trick
[00:10:10] is if I know it's a there's a certain
[00:10:12] pattern to the schema that I'm always
[00:10:15] going to want to do joins a certain way
[00:10:16] then I can throw away anything that's
[00:10:18] not that. There's a bunch of heristics
[00:10:19] you apply to prune down the search base.
[00:10:21] We're not covering that here. That's
[00:10:23] more of the advanced class stuff. Um but
[00:10:25] there's other like you can do um you can
[00:10:28] run like an approximation algorithm to
[00:10:30] get like a dirty start and then use that
[00:10:33] as the like you know dirty heristic to
[00:10:35] like kind of get some initial setting
[00:10:37] and then use that as the starting point
[00:10:38] rather than from starting from scratch.
[00:10:40] But we have to understand the the basic
[00:10:41] algorithm before we do start doing the
[00:10:42] advanced stuff. But the answer is right
[00:10:43] yes this gets huge. Postgress does this
[00:10:46] except if you have by default if you
[00:10:49] have 13 or more tables in your query.
[00:10:52] They don't do this. They fall back to a
[00:10:53] genetic algorithm which I'm I'm not
[00:10:56] going to cover. I if I have time I can
[00:10:57] show slides but like that thing is like
[00:10:59] super broken and it doesn't work. Right?
[00:11:02] But that's what they do. It's basically
[00:11:03] an approximation because you can't do
[00:11:05] sort of this dynamic programming.
[00:11:08] >> What's that?
[00:11:10] >> His question is where's the number 13
[00:11:11] come from?
[00:11:13] magic constant, right? Like you can you
[00:11:15] can change it. By default, it's 13. Some
[00:11:17] people just turn it off entirely because
[00:11:18] it doesn't always it's not guaranteed
[00:11:20] good results. Yes.
[00:11:22] >> Is there aial
[00:11:24] version of the algorithm that works?
[00:11:26] >> The question is uh is there a potential
[00:11:28] version of algorithm that works? Um
[00:11:31] yeah. So I'm not there is a class of of
[00:11:36] optimization algorithms that basically
[00:11:37] are doing that are random like you can
[00:11:40] do simulated kneeling. You can do the
[00:11:42] the genetic algorithm from Postgress
[00:11:44] that's sort of the same thing like
[00:11:45] you're rolling the dice and see what
[00:11:46] comes up like again you can't guarantee
[00:11:48] you're going to find the optimal because
[00:11:49] it's random like a random walk through a
[00:11:51] search space. Uh in the case of the
[00:11:53] Postgress one I mean it's a it's several
[00:11:56] factors right like is the implementation
[00:11:57] of the algorithm correct I think I think
[00:12:00] they said there was a bug I forget what
[00:12:01] it is like it's not truly random um and
[00:12:04] then it also depends on your cost model
[00:12:05] which we'll get in a second. If your
[00:12:06] cost model is crap, then who cares how
[00:12:08] good the algorithm is? Like you just
[00:12:09] it's gonna make bad predictions. But if
[00:12:11] you're consistently crappy, then then
[00:12:12] maybe that's okay. Give me give me half.
[00:12:15] We'll get to that. Yeah.
[00:12:18] Yeah. The postcest one is the only one I
[00:12:20] know that does the random walk. The
[00:12:21] Germans do um if they if it's a less
[00:12:24] than 100 tables, they have an efficient
[00:12:26] implementation of the algorithm that
[00:12:27] they can find. They'll do the exact
[00:12:29] search, the exhaustive search above 100.
[00:12:31] Then they do uh they do sort of like the
[00:12:34] the the approximation first and then
[00:12:37] that seeds a algorithm that does the
[00:12:39] more search. But that way you know
[00:12:40] you're throwing away things that you're
[00:12:42] never going to you know consider anyway.
[00:12:45] [snorts]
[00:12:46] All right. So the the basic the walk
[00:12:48] through the algorithm looks like this.
[00:12:49] So again at the bottom is my starting
[00:12:51] point and I want to get to the top here.
[00:12:53] So I'm going to work my way to the top
[00:12:54] and say okay I have three tables artist
[00:12:56] album appears. assuming I've already
[00:12:58] picked out what the the access method
[00:13:00] I'm going to use. So now I got to figure
[00:13:01] out in what order I want to join these
[00:13:03] tables and what what physical operator
[00:13:06] what algorithm I'm gonna want to use
[00:13:08] hash joins versus sort join. Uh you know
[00:13:11] you'd also maybe consider nestloop join
[00:13:12] as well but for space reasons I'm not
[00:13:14] showing that and again I'm only showing
[00:13:15] certain uh you know three different
[00:13:17] combinations. This obviously would span
[00:13:18] out over here but it wouldn't fit in
[00:13:20] PowerPoint. So for now each of these
[00:13:22] pass up into the next level in the tree.
[00:13:24] I would say okay this one over here is
[00:13:26] joining artist and appears and I haven't
[00:13:28] joined album yet. So I can either join
[00:13:30] do a hash join to join artist and
[00:13:32] appears. I can do an emerge join to do
[00:13:34] artist and peers. And for each of those
[00:13:35] now I can cost them use my cost model
[00:13:37] and say which of these ones are going to
[00:13:39] have a lower cost. And so for each of
[00:13:41] these pass up to the next level in the
[00:13:43] tree I'll I'll keep the one that has the
[00:13:45] lowest cost and throw away the others.
[00:13:48] And now at the next level I say I do the
[00:13:50] same thing. So again, I've I've already
[00:13:52] joined artisan peers. Now I want to take
[00:13:53] the output of artist peers join and join
[00:13:55] it with album. So I'm going to do the
[00:13:57] same thing and look at all the possible
[00:13:58] combinations I have for different
[00:14:00] physical operators to do do that next
[00:14:01] level of the join. Then go back and cost
[00:14:04] each of them to figure out which one has
[00:14:05] the lowest cost going up to the to this
[00:14:07] the same level coming or coming from the
[00:14:09] same parent going up to the next level.
[00:14:12] Right? Throw away throw away all the
[00:14:13] other ones. And then now for each of
[00:14:15] these paths up to the root or the top of
[00:14:19] the query plan, I'm going to pick which
[00:14:20] whatever one has the the lowest global
[00:14:22] cost across all these different choices,
[00:14:25] right? And then that's that's what
[00:14:27] you're deciding to be the lowest cost.
[00:14:29] Like it's divide and conquer. Rather
[00:14:30] than looking at all possible
[00:14:30] combinations, I'm just looking at each
[00:14:32] level like, okay, if we just join two
[00:14:33] tables, what's the different what's the
[00:14:35] different possibilities I have? And now
[00:14:37] you can kind of see where some of these
[00:14:38] heristics could help prune the search
[00:14:40] base because you would say like well I
[00:14:43] going back here like if I if I know that
[00:14:46] the tables are so big that I'm never
[00:14:48] want to do a nestloop join don't even
[00:14:49] bother trying to add a nest loop join
[00:14:51] and considering that right. So back to
[00:14:53] his point there's more magic constants
[00:14:54] you could say like if the table's this
[00:14:56] size then don't consider nest loop
[00:14:58] joint.
[00:15:01] So another hack at all this is that
[00:15:05] the
[00:15:06] one is that instead of trying to so
[00:15:09] instead of when we keep track of the
[00:15:11] best plans
[00:15:13] we we can ignore in what form we need
[00:15:18] the data to be in meaning like what are
[00:15:20] the physical properties of the data like
[00:15:21] sorting for example sorting is a
[00:15:23] physical property of data so in this
[00:15:24] case here I cannot consider any of that
[00:15:27] while I'm doing my like computing my
[00:15:29] joint order and then I can go back and
[00:15:31] then say after I've computed the the
[00:15:33] join uh the join order if I recognize
[00:15:36] that there isn't it isn't in the
[00:15:38] property that I need like I need the
[00:15:39] data sort of in the artist ID uh if it's
[00:15:42] not if it's not like that then I can
[00:15:44] just graft on a sort at at the top up
[00:15:46] above or I can modify my cost model and
[00:15:49] recognize that going back here the the
[00:15:52] the mer sort merge join is going to put
[00:15:54] the data in the sort order that I need
[00:15:56] so I'll make sure that has a lower cost
[00:15:59] than the the hash join,
[00:16:02] right? The system R system R1 is pretty
[00:16:04] primitive, but the later ones you can
[00:16:06] consider this directly an end. Yes,
[00:16:07] question.
[00:16:09] All right, so this is bottom to the top
[00:16:11] top down again, you're starting with the
[00:16:13] the the logical plan of what would be
[00:16:15] the final output and then now you're
[00:16:16] basically doing branch and bound search
[00:16:18] looking for uh a path down to to all the
[00:16:21] different leaf nodes so that you can
[00:16:22] have a complete query plan. Right? So
[00:16:25] the system R bottom to the top approach
[00:16:28] that was embedded in the 1970s. This
[00:16:30] approach came about in the late 80s
[00:16:33] early 90s. Um the the most famous
[00:16:37] implementation of this is something
[00:16:38] called cascades uh which is implemented
[00:16:40] in SQL server green plum and cockroach
[00:16:44] and I think data bricks might be using
[00:16:45] this in their catalyst optimizer as well
[00:16:47] at least some some form of this. Um, and
[00:16:49] the basic idea is that as you're as
[00:16:51] you're traversing down and looking at
[00:16:53] different options in the query plan, uh,
[00:16:55] you keep track of the best plan you've
[00:16:56] seen so far. And then at some point, if
[00:16:58] you reach a path in the query plan where
[00:17:00] the current cost, you know, from from
[00:17:02] the route to where you're at now, if
[00:17:04] that's greater than the than the lowest
[00:17:06] cost you've seen for a complete query
[00:17:08] plan, then you know, you don't need to
[00:17:09] keep going down in in that, you know,
[00:17:11] searching that part of the tree. You can
[00:17:13] just sort of cut it off and not have to
[00:17:14] look at everything. But the way you're
[00:17:16] sort of
[00:17:18] re potentially revisiting uh different
[00:17:21] choices uh this can get uh quite
[00:17:24] expensive because you could be
[00:17:25] revisiting things over and over again.
[00:17:26] So all these implementations are re rely
[00:17:29] on a memo table to or memorization table
[00:17:31] to keep track of like here's things I've
[00:17:32] costed before don't cost them again.
[00:17:36] So the for the longest time the only
[00:17:39] sort of there was only a few scattered
[00:17:41] research papers that describe how to do
[00:17:43] this approach and in especially in the
[00:17:45] context of this thing called cascades.
[00:17:47] Um, but it was well known that SQL
[00:17:49] Server had been doing this since the
[00:17:50] 1990s. Like the guy that wrote that B+
[00:17:52] paper we talked about before when we
[00:17:54] talked about the paralization stuff, the
[00:17:55] volcano and exchange operators. That's
[00:17:57] this one same guy got hired at Microsoft
[00:18:00] in the late 90s. They threw away the old
[00:18:01] core they had and he rewrote it to be
[00:18:04] the the cascade style. And it probably
[00:18:06] is, you know, with with the exception
[00:18:08] maybe maybe the Germans, um, it probably
[00:18:11] it is the best choreizer. Certainly the
[00:18:13] best top down core optimizer. Germans
[00:18:15] have the best bottom up. And so for the
[00:18:17] longest time there wasn't a good de
[00:18:18] description of how to actually implement
[00:18:20] one of these optimizers till Microsoft
[00:18:22] put out a book last year. It's open
[00:18:24] source. You you can go down for free and
[00:18:26] read it. That thing's amazing. It's the
[00:18:27] best book I've read last year in my
[00:18:28] life. Um like it describes all the
[00:18:31] detail not not all but many of the
[00:18:33] details of how SQL server does uh
[00:18:35] implements their query optimizer. But
[00:18:37] obviously we can't go too much detail in
[00:18:39] for this lecture but you know we take
[00:18:40] the advanced class we'll spend more time
[00:18:42] talking about this. All right. So again,
[00:18:44] top down obsation, we're start with what
[00:18:46] we want the the final outcome to be,
[00:18:48] right? So we want our final outcome in
[00:18:49] this example here. Now we're going to do
[00:18:51] the artist appears and album. And now
[00:18:52] we're also going to include the the
[00:18:55] physical property we want as well. We
[00:18:56] want the data to be sorted by the artist
[00:18:58] ID. So now we're going to go through and
[00:19:01] apply a bunch of these transformation
[00:19:02] rules to either convert logical plans to
[00:19:05] other logical plans or logical plans
[00:19:07] into physical plans, right? We never go
[00:19:09] back from phys from physical back to
[00:19:10] logical. So here's sort of again similar
[00:19:13] to what we had before. Here's all the
[00:19:14] stages of the of the sort of logical
[00:19:16] operators we have down below. And we
[00:19:18] would run we would run uh transformation
[00:19:21] rules to expand all all these things
[00:19:22] out. And then now starting from the
[00:19:24] root, we're going to go through and look
[00:19:25] at uh the different choices we would
[00:19:27] have to get to this final outcome up
[00:19:29] here. So we know that we want to join uh
[00:19:32] artist appears and album. So we say down
[00:19:34] below we we can join artists and appears
[00:19:38] uh after joining that and then we join
[00:19:40] with with uh with album and then this
[00:19:42] one we use a merge join. So now instead
[00:19:44] of looking at all the possible
[00:19:45] combinations at the next level we
[00:19:47] actually traverse down into this and
[00:19:49] look at all the ways we would get to the
[00:19:51] that level going up and keep going down
[00:19:53] here. So now we say are we want to join
[00:19:55] artist and appears. So I can either do a
[00:19:57] hashing for that and I keep going down
[00:19:58] here and now we say how do I actually
[00:20:00] access these tables right and I would
[00:20:02] pick my my access method for that. But
[00:20:03] then now once I sort of reach the
[00:20:05] bottom, I then go back up and start
[00:20:07] looking down another path because I'm
[00:20:08] doing death first search. I got to get
[00:20:10] to the bottom first before I start
[00:20:11] looking at at other levels. So then I
[00:20:14] come in artist appears and I can do
[00:20:16] possibly a merge join of that. Do the
[00:20:18] same thing. Come down and and figure out
[00:20:20] what the access methods are for these
[00:20:21] other operators. And this is where the
[00:20:23] memorization stuff helps because I'm
[00:20:25] revisiting like how do I want to access
[00:20:27] artists albums and appears at the lowest
[00:20:28] level in the trees as I'm calling and re
[00:20:30] coming back down over and over again. I
[00:20:32] don't want to compute that cost every
[00:20:34] single time I revisit these things. So I
[00:20:36] just check the memo table to see whether
[00:20:37] I' I've seen I've visited this node
[00:20:39] before.
[00:20:41] All right. So you basically do that.
[00:20:43] Imagine this thing again standing out
[00:20:45] and getting getting quite large. And
[00:20:46] then there are without going too much
[00:20:49] details there's priorities you can set
[00:20:51] for the for these transformation rules
[00:20:55] so that maybe you want to look at hash
[00:20:56] joins before you look at merge joins.
[00:20:58] Right? in case you run out of time in in
[00:20:59] your in your in your search, you at
[00:21:01] least have a plan that generates a bunch
[00:21:03] of hash joins. Even though it may not be
[00:21:04] optimal, but it's it's good enough.
[00:21:06] The other thing we have to do is is keep
[00:21:08] track of how we're going to make sure
[00:21:09] that the data has the physical property
[00:21:11] we want at at the top of the of the
[00:21:13] plan, right? Or actually really any
[00:21:15] level of the plan, right? We have this
[00:21:17] this this order by requirement that the
[00:21:19] data has to be sorted on artist ID. So
[00:21:21] now if we go and look at like this
[00:21:23] operator here to do a hash join, we
[00:21:26] would know that that doesn't guarantee
[00:21:28] that data is in that property that we
[00:21:29] want. So we would not we know we don't
[00:21:31] even need to need need to actually go
[00:21:33] look at it,
[00:21:35] right? But then if we add a sort
[00:21:36] operator, it doesn't do the join for us,
[00:21:39] but it does put the data in the in the
[00:21:40] in the in this the form that we need. So
[00:21:43] we're allowed to go visit that and then
[00:21:44] again apply more transformation rules to
[00:21:46] say, okay, well, I still need to join
[00:21:48] this data. How how am I getting it uh up
[00:21:50] to me? Right? So then now if I do a
[00:21:52] transformation rule and say now add the
[00:21:54] hash join when I cost it and I would see
[00:21:57] that oh adding the sort plus the hash
[00:21:59] join that's that cost is greater than
[00:22:01] the merge join path I seen before I
[00:22:04] don't even even consider it again going
[00:22:06] down.
[00:22:10] So maybe I'll skip this, but I just want
[00:22:11] to say that like the way they're going
[00:22:13] to implement the the
[00:22:15] uh to guarantee the data is in in the
[00:22:17] physical property you want. They just
[00:22:18] add these enforcer nodes and it's just a
[00:22:20] way to say like make sure that nothing
[00:22:23] comes up to me that isn't ordered the
[00:22:25] way that I need it up above. And so I
[00:22:27] can do a bunch of transformation rules
[00:22:28] and I can I can match them on uh in the
[00:22:32] rules it would say this data will be in
[00:22:34] this form that you need or this data
[00:22:35] would not be in the form that you want.
[00:22:37] And so I only apply the rule the the
[00:22:38] rules that I know that I want. Then I
[00:22:39] have avoids again having to fan out and
[00:22:41] look at possibly everything. Right? So
[00:22:43] in the sake of time I'll just skip this.
[00:22:45] I'm just trying to say that like you you
[00:22:46] can you can add this in uh as part of
[00:22:49] this first class property in the search.
[00:22:53] Okay. So that's the search the search
[00:22:55] algorithms. Again we go from we can
[00:22:57] either just do pure heristics and just
[00:22:58] apply them uh one after another until we
[00:23:00] put the data in the form that we or the
[00:23:02] query in a in a form that we actually
[00:23:03] execute. If we're going to do costbased
[00:23:05] search, then we either go top down
[00:23:07] versus bottom up. Most systems do bottom
[00:23:09] up. The the more sophisticated, more
[00:23:12] more newer ones are doing uh top down,
[00:23:14] but it's super hard either either way.
[00:23:17] All right, so
[00:23:20] for all these different operators, when
[00:23:22] we talked about before, when we talked
[00:23:23] about query execution,
[00:23:25] we talked about different ways or
[00:23:28] different formulas we'd use to compute
[00:23:29] the amount of work they were going to
[00:23:30] do. Like when we talked about the
[00:23:32] different join algorithms, the sort
[00:23:34] merge joint, nestloop join, and the hash
[00:23:35] joins, we talked about them in terms of
[00:23:38] how much data they were going to have to
[00:23:39] read in and how much data I got to write
[00:23:41] out, like how how many IO's they were
[00:23:42] doing. But the challenge is going to be
[00:23:44] now that we have uh you know, now that
[00:23:48] we're trying to build a real query plan,
[00:23:50] we have real data that we're going to be
[00:23:53] running on. It's no longer these sort of
[00:23:55] abstract numbers or these formulas. we
[00:23:56] actually have to come up with you know
[00:23:58] estimates for what this data is actually
[00:24:00] going to be. So like you know when we
[00:24:02] talk about those IO's how are we
[00:24:04] actually computing them other than just
[00:24:05] me showing them on PowerPoint right and
[00:24:08] this is going to be hard to do because
[00:24:10] as we'll see the the the
[00:24:13] amount of data you may have to process
[00:24:16] if you're you know a join operator it's
[00:24:18] going to depend on how much data is
[00:24:20] being sent to you below you in in the
[00:24:21] query plan
[00:24:23] right and then the problem is going to
[00:24:25] get compounded because if I have
[00:24:27] multiple joins in my query plan right I
[00:24:29] can maybe do okay predict icting how
[00:24:31] much data is coming into me from the
[00:24:33] scanning tables to my join. But now I
[00:24:35] take the output of that join and feed it
[00:24:36] into another join. Right? If my estimate
[00:24:39] for how much data is coming out of my my
[00:24:40] first join is off, then I have no, you
[00:24:43] know, I have no look uh I have no hope
[00:24:46] picking out a good estimate for how much
[00:24:47] data is coming out of the second joint,
[00:24:49] right? and for all these operators.
[00:24:52] Again, we can't just go execute the
[00:24:53] queries because if we could do that,
[00:24:54] then we could just, you know, we just
[00:24:56] run the queries instead of trying to
[00:24:58] figure out, you know, for the thousands
[00:25:00] or even tens of thousands, millions
[00:25:01] different possibilities that I have, I
[00:25:03] can't each run those each individually.
[00:25:05] [snorts]
[00:25:06] So, this is where the the data systems
[00:25:08] cost model is going to come in. We're
[00:25:09] going to way it's a way for us to
[00:25:10] estimate the the the the amount of work
[00:25:14] an operator is going to have to do
[00:25:16] within a query plan given the current
[00:25:18] state of the database. And then what I
[00:25:20] mean the current state is like what the
[00:25:22] what the distribution of values look
[00:25:24] like in the in the tables that you're
[00:25:27] trying to trying to access a query
[00:25:28] against. [snorts] And the the challenge
[00:25:30] of this is that the in some database
[00:25:33] systems or some applications the data is
[00:25:35] not not static. Meaning, if I'm running
[00:25:37] a website like Reddit, I'm getting new
[00:25:40] updates and new inserts all the time,
[00:25:42] and potentially the distribution of
[00:25:44] values could change. So, my cost
[00:25:45] estimates from today may look a lot
[00:25:47] different than what they might look like
[00:25:49] a month from now. They're usually not
[00:25:51] too drastic, but like it's it can drift
[00:25:54] over time and cause problems. And as I
[00:25:57] said before, this this internal cost
[00:25:58] that we're generating is not going to be
[00:26:00] something that's related to the physical
[00:26:01] world. Usually, usually it's not some
[00:26:03] systems, but it's rare. it is uh it's
[00:26:06] gonna be really it's going to be this
[00:26:08] relative number that we just use to say
[00:26:10] whether one query plan is going to be
[00:26:12] more efficient or better than than
[00:26:13] another query plan and we can't take you
[00:26:15] know what our postgress estimates are
[00:26:17] and apply them to my SQL optimizer
[00:26:19] they're completely separate notions.
[00:26:23] So the two types of components we would
[00:26:25] have in our cost estimates for an
[00:26:26] operator are be the physical cost and
[00:26:28] the logical cost. So physical cost would
[00:26:30] be the resource consumption of the of of
[00:26:34] of an operator of a query plan when the
[00:26:37] data runs that query right basic things
[00:26:41] we already talked about was like IO's
[00:26:42] how much how many blocks of data or
[00:26:43] pages of data I'm going to read from
[00:26:45] from from disk or or write to disk but
[00:26:48] you can extend that out for other parts
[00:26:50] of of a of a computer right how many how
[00:26:53] many packets I'm going to send over the
[00:26:55] network how much time I'm going to spend
[00:26:56] in the CPU how many cycles to to do you
[00:26:59] some operation. Um cache measures are a
[00:27:02] little bit more complicated to the to
[00:27:04] estimate by getting more low level in
[00:27:06] part parts of the hardware it becomes
[00:27:07] more problematic. Um and of course
[00:27:10] obviously this is going to depend on how
[00:27:11] much uh you know what what my hardware
[00:27:14] actually looks like what my CPUs look
[00:27:15] like how much C you know L3 cache do I
[00:27:17] have. [snorts] So typically most systems
[00:27:19] are going to do uh IO's as the as the
[00:27:23] the dominant factor. You maybe also want
[00:27:25] to include how much memory I'm going to
[00:27:27] consume in my operator because obviously
[00:27:28] that's a finite resource. For CPU
[00:27:31] cycles, the high-end systems can be a
[00:27:33] bit more smart about this. The something
[00:27:35] like Postgress sort of say, you know, if
[00:27:38] something's in memory, what's the
[00:27:39] relative cost of of of
[00:27:42] accessing something in memory,
[00:27:43] processing in memory versus like reading
[00:27:45] something from disk? That's a a loose
[00:27:47] approximation for for for you know
[00:27:50] coming in and out.
[00:27:53] logical cost would be the estimated
[00:27:55] output size per operator. And what's
[00:27:57] what's nice about this at least logical
[00:27:59] cost is that no matter what physical
[00:28:02] operator I choose, the logical cost has
[00:28:04] to be the same, right? So if I'm joining
[00:28:07] table A and table B, they're you know,
[00:28:09] no matter whether you do a servers join
[00:28:11] or nest loop join or a hash join, you
[00:28:13] know, it's going to produce x number of
[00:28:14] tupils.
[00:28:16] So sometimes there's logical costs can
[00:28:18] be used to prune things out before
[00:28:19] you're actually looking at all the
[00:28:20] different physical operator physical
[00:28:21] operator choices for a uh for for a
[00:28:26] query plan. But where the problem is in
[00:28:28] order for me to predict how much data my
[00:28:30] operator is going to spit out I got to
[00:28:31] know how much data is coming in and now
[00:28:33] now you get this sort of recursive
[00:28:34] problem we'll see later on. [snorts]
[00:28:38] So just showing postress as an example
[00:28:40] again this is this I would say this is
[00:28:41] the most basic simplest cost model you
[00:28:44] can have. So what they care about is
[00:28:47] again the CPU cost and then the relative
[00:28:50] cost for different different types of IO
[00:28:52] I might might perform like random IO
[00:28:54] versus sequential IO and then you weight
[00:28:56] them these different factors based on
[00:28:59] some magic thing magic constant you can
[00:29:01] specify in the in your configuration of
[00:29:03] the database system right so they'll say
[00:29:05] if something's in memory that's be 400
[00:29:07] 400 times faster than reading something
[00:29:10] from from a disk and if you're reading
[00:29:12] something that's sequential IO reading a
[00:29:14] sequence of pages that are that are one
[00:29:15] after another that's going to be 4x
[00:29:17] faster than doing random IO.
[00:29:20] So if you go look at the postgress
[00:29:21] documentation you you'll see all the
[00:29:23] definition of these parameters like
[00:29:24] sequential scan cost random page cost
[00:29:27] these are you know these are numbers you
[00:29:29] can specify these waiting factors but it
[00:29:31] has this huge blurb right here that says
[00:29:33] basically warn you saying hey look these
[00:29:35] are kind of just like you know there
[00:29:37] isn't a good way to exactly determine
[00:29:39] these values because it changes per for
[00:29:41] for different hardware
[00:29:43] and so they're sort of meant to be a you
[00:29:45] know a good guess estimate of what you
[00:29:48] should do right And if you change them
[00:29:50] too much, they say you have problems
[00:29:52] because you end up picking bad query
[00:29:53] plans. So this is like I would say
[00:29:56] postgress cost model is not very
[00:29:57] sophisticated. It's the bare minimum you
[00:29:59] would need to have a say you have a a
[00:30:00] cost model in your data system. The
[00:30:02] high-end systems like DB2 and Oracle and
[00:30:05] those guys, they do all sorts of tricks
[00:30:07] to try to figure out what these actual
[00:30:08] values are. Like when when you turn on
[00:30:10] DB2 from IBM, they'll run a bunch of
[00:30:12] microbenchmarks to figure out what is
[00:30:14] the cost of reading something from disk,
[00:30:15] what is the cost like what is the actual
[00:30:17] time it takes to send a packet over the
[00:30:19] network? how fast is my CPU and they use
[00:30:21] that in their cost model instead of
[00:30:23] these magic constants that you set in in
[00:30:25] Postgress, right? You can still override
[00:30:27] that but it's trying to compute things
[00:30:28] for you uh on their own
[00:30:32] because that's trying to reflect the
[00:30:32] real hardware. So again, if your
[00:30:33] database moves from one machine to
[00:30:35] another, the they'll automatically
[00:30:36] update. Yes.
[00:30:41] >> Their question is their question is why
[00:30:43] doesn't Postgress do something more
[00:30:44] sophisticated?
[00:30:46] It's hard. Nobody and also too like
[00:30:49] create apper like nobody wants to touch
[00:30:50] it in postgress [snorts] because it's
[00:30:52] it's very brittle
[00:30:54] right um I mean like there's
[00:31:01] I would say it would be it would be
[00:31:04] easier for them to add add the sort of
[00:31:06] DB2 calculations that I mentioned the
[00:31:08] micro benchmarks that's easier thing to
[00:31:10] do than rewriting the whole like the
[00:31:13] whole engine it's like whole the query
[00:31:14] oper
[00:31:16] >> that would be a major overhaul
[00:31:18] >> [snorts]
[00:31:21] >> Yes. I mean, if you ever like looked in
[00:31:24] Linux, like if you ever looked like
[00:31:25] slroc CPU info or do lscpu, you'll see
[00:31:28] see a thing called bogo myips, right?
[00:31:30] That's a microbenchmark when Linux
[00:31:32] starts up that that when you so that it
[00:31:34] uses that as the timing for like
[00:31:36] scheduler, right? It's it's
[00:31:38] approximation how the hardware actually
[00:31:39] is, right? [snorts] I would say also too
[00:31:41] postgress like in the like the high-end
[00:31:43] systems DB2 like they're distributed
[00:31:45] parallel systems. So like they need to
[00:31:47] worry about like how much takes time you
[00:31:49] know send data over the network and
[00:31:52] because they can also have uh be aware
[00:31:54] of maybe storage taring like fast local
[00:31:57] storage versus remote storage. They need
[00:31:59] to know what those times are as well.
[00:32:00] Post has no notion of those things. It
[00:32:02] just has table spaces.
[00:32:04] Okay. So the way we're going to get
[00:32:08] these estimates of how much work we
[00:32:09] think our operator is going to do,
[00:32:10] right? how much how many data you think
[00:32:12] we're going to process is going to be
[00:32:14] through statistics.
[00:32:16] And so when people talk about in your
[00:32:18] data system you they're keeping track of
[00:32:19] statistics, it's basically an internal
[00:32:21] metadata that the data center is going
[00:32:23] to collect on your data. They then you
[00:32:26] use in the query optimizer to try to
[00:32:27] figure out what is the again expected
[00:32:29] work for either logical level or the
[00:32:32] physical level for the for the different
[00:32:33] operators, right? And again, we we have
[00:32:36] we need to have these summaries because
[00:32:38] we can't actually just run our queries
[00:32:40] on the real data. Although we can do
[00:32:41] sampling. We'll see that in a second.
[00:32:43] But like you can't run the full query
[00:32:44] because again by by that point I've
[00:32:47] already produced the answer I you know I
[00:32:48] would want to run the query and getting
[00:32:50] the wrong join order. You know sometimes
[00:32:52] you can be a thousandx worse than than
[00:32:54] picking the optimal plan. So we want to
[00:32:56] avoid all that. Of course now the
[00:32:58] challenge is nothing comes for free as
[00:33:00] it often the case in systems and data
[00:33:02] system and computer science. So there
[00:33:04] would be this trade-off of how accurate
[00:33:05] you want our our statistical summaries
[00:33:07] to be versus how how much time you want
[00:33:10] to spend computing them, how much
[00:33:11] storage space you want to maintain or
[00:33:13] use to maintain these things. Uh the you
[00:33:17] know when things get updated in the
[00:33:19] database, do I stop what I'm doing
[00:33:20] immediately and go update my statistics
[00:33:23] or am I allowed to get a little bit out
[00:33:24] of date and you know sort of suffer
[00:33:26] those consequences, right? So, so
[00:33:28] there's no one way there's no one sort
[00:33:30] of set of criteria I can say this is
[00:33:32] what you always want to do. It depends
[00:33:34] on so many different factors
[00:33:37] but most systems sort of implement um
[00:33:41] they sort of implement one sort of uh
[00:33:44] approach to doing statistics
[00:33:46] SQL server and the high-end systems can
[00:33:47] do more but like like in Postgress
[00:33:49] there's only you know they have pretty
[00:33:50] basic statistics and you either run them
[00:33:53] which next slide you either run them
[00:33:54] when you ask it to or you run it when
[00:33:56] you do background maintenance things
[00:33:58] right
[00:34:00] all right so again the way we're going
[00:34:01] to store these statistics is that
[00:34:02] they're just going to be another table
[00:34:04] in our database, right? It's a special
[00:34:06] table, an internal table in the catalog,
[00:34:08] but we're not going to store this in in
[00:34:10] memory in a heap in separate space.
[00:34:12] We're just stored as a regular tupil. So
[00:34:14] that way we get all the the the storage
[00:34:16] and durability guarantees that we'd have
[00:34:18] in a you know as regular data. And as I
[00:34:22] said, I already said before like the
[00:34:24] when the
[00:34:26] when the data what triggers the data
[00:34:27] center to go collect the statistics
[00:34:28] depends on the implementation. It could
[00:34:31] be things like you know at like in
[00:34:34] Oracle's case I think it's like 10 pm at
[00:34:36] night they just run a job that you know
[00:34:37] automatically go collect new statistics
[00:34:39] for the day right for the next day right
[00:34:41] in Postgress if you modify a table so
[00:34:44] much by I think like 10% or 20% then
[00:34:46] that goes ahead and triggers off the the
[00:34:48] the background job to go collect
[00:34:50] statistics and again you can manually do
[00:34:52] this using either in post it's analyze
[00:34:54] in other systems called update
[00:34:56] statistics so let me go a quick demo of
[00:35:00] but they that you can see these things
[00:35:02] just to prove to you that they're just
[00:35:03] regular tables. Um, and there's nothing
[00:35:07] nothing special about them.
[00:35:09] Uh, let me log in.
[00:35:15] So,
[00:35:20] turn off the lights. Although that might
[00:35:21] kill the camera, but whatever.
[00:35:26] I probably should do that. Whatever. All
[00:35:28] right. So, this is Postgress. Um so
[00:35:30] Postgress has this table called PG
[00:35:33] statistic. It's a again anything with
[00:35:36] underscore PG is um
[00:35:39] like and a name like that that's that's
[00:35:41] an internal catalog for for Postgress,
[00:35:43] right? So I see a bunch of stuff. What
[00:35:46] does this actually all mean? So let's do
[00:35:47] it on on a per table basis. So remember
[00:35:50] before I had this enroll table um in my
[00:35:54] examples, right?
[00:35:59] Right? This had, you know, five five
[00:36:01] records in it keeping track of what
[00:36:03] grades of people was in the class.
[00:36:04] Right? So I can go to PG statistic
[00:36:11] and say give me the give me the
[00:36:13] statistics for for this table. Right? So
[00:36:16] that's kind of hard to read, but so let
[00:36:17] me put it in this sort of extended form.
[00:36:20] Right? So when you think of like each
[00:36:22] block here is going to be a you know is
[00:36:24] is one row in the table. So what do we
[00:36:27] see? We see that we have the the table
[00:36:29] name enrolled. We have the attribute
[00:36:30] name, the column name, SID. Then we have
[00:36:33] some other stuff about like the number
[00:36:34] of distinct values, most common values,
[00:36:36] most common frequencies. They don't have
[00:36:38] any histograms here because there's not
[00:36:40] a lot of data. But you can see like
[00:36:42] they're keeping track of a bunch of
[00:36:43] things and they're just rows and arrays
[00:36:45] in in in the table, right? And so if I
[00:36:49] go ahead and kill this,
[00:36:52] let's just delete the the stats for
[00:36:56] uh you know for this table. Now when I
[00:36:58] go back and try to read it, right,
[00:36:59] nothing shows up. So if I tell Postgress
[00:37:02] to
[00:37:03] uh run analyze on that table, take 23
[00:37:06] milliseconds because it's you know it's
[00:37:08] fits on one page. And then now when I go
[00:37:10] back, I see again all the same stats as
[00:37:11] before. And this is what we're going to
[00:37:12] use in our in our cost model to figure
[00:37:14] out how much data actually exists uh
[00:37:17] when I when I do estimations. So just
[00:37:19] give you what a
[00:37:21] what a real table might look like with
[00:37:23] real stats. So this is from the our
[00:37:25] encyclopedia dbdio. Um [snorts] now you
[00:37:29] can see see now you have this histogram
[00:37:30] bounds. You have uh other here the
[00:37:34] histogram bounds. There's no frequent
[00:37:35] values for any of these but like you you
[00:37:39] basically see like again it's storing
[00:37:41] this one is for is current true false
[00:37:43] and you have the frequency of of those
[00:37:47] right most of the data in this is where
[00:37:49] the is current is set to false. So
[00:37:51] again, it's just there's nothing magic
[00:37:53] about this. They're just data structures
[00:37:55] stored in uh s store sort as a regular
[00:37:59] table,
[00:38:00] [snorts] but it's not something you
[00:38:02] should be maintaining or doing yourself.
[00:38:03] Like the data should just do this for
[00:38:04] you,
[00:38:06] right? And this is what I've already
[00:38:07] said before like so for every single
[00:38:09] column in a table, the data system is
[00:38:11] going to go try to collect these
[00:38:13] statistics and basically maintain some
[00:38:14] histograms and other other things we'll
[00:38:16] talk about in a second. Um so sometimes
[00:38:20] in some systems they'll also track the
[00:38:22] statistics for groups of columns or
[00:38:23] groups of attributes. These are called
[00:38:25] correlated statistics. So if I know that
[00:38:29] uh like if there's a correlation between
[00:38:32] like zip code and city like every city
[00:38:34] can only be in one zip code. So instead
[00:38:37] of treating them as independent uh
[00:38:39] variables I know they're correlated and
[00:38:41] I can compute stats on those combination
[00:38:43] of those two columns together. So then
[00:38:45] when I do my estimations I get I get
[00:38:47] better better uh better results.
[00:38:50] So the in in SQL you can say create
[00:38:53] correlated statistics you can tell it I
[00:38:54] want to create statistics on multiple
[00:38:56] columns. Uh in some systems like SQL
[00:38:58] server if you put two columns together
[00:39:00] in an index this the data system says oh
[00:39:03] well they must be important they must
[00:39:04] must go together so let me compute the
[00:39:06] correlated statistics uh for them
[00:39:08] together. Right? Otherwise, because it's
[00:39:11] it's, you know, it's it's it's
[00:39:13] combinatoral the number of different
[00:39:14] choices you have of correlated
[00:39:15] statistics. You don't want to generate
[00:39:17] all these things. That'd be sensitive to
[00:39:18] do. So, they try to use tricks to figure
[00:39:20] out which one they actually combine
[00:39:21] together.
[00:39:24] All right. So, there's be four
[00:39:25] approaches for how we want to maintain
[00:39:27] our statistics, right? Our four
[00:39:30] different type of statistical data
[00:39:31] structures. The first is going to be
[00:39:32] histograms. This most common thing this
[00:39:34] what we saw in Postgress and this just
[00:39:37] for each occurrence of a unique value
[00:39:38] within a column count the number of
[00:39:40] times that it shows up or you can put
[00:39:43] bucketing them together and we'll see
[00:39:45] that in a second. [snorts]
[00:39:47] Uh another one would be sketches. You've
[00:39:50] already saw in a sketch in project zero
[00:39:51] the count min sketch and these are
[00:39:53] statistical approximations or sorry
[00:39:57] probabilistic data structures that can
[00:40:00] approximate the different factors
[00:40:02] aspects of the data. Sampling would be
[00:40:05] when we t maintain a small snapshot or
[00:40:08] small subset of the original tables
[00:40:12] uh and use that to estimate statistics
[00:40:14] or in some cases maybe run like many
[00:40:16] queries against the real data and then
[00:40:18] try to extract the statistical
[00:40:20] properties of the data uh from that
[00:40:22] sample. And then the last one would be
[00:40:25] the more more modern techniques using
[00:40:27] machine learning models that try to
[00:40:29] learn the cardality current learn the
[00:40:31] the statistical properties of of tables.
[00:40:34] So in terms of what systems implement
[00:40:37] what the very top histogram is the most
[00:40:40] common one because they're the easiest
[00:40:42] in the in the modern era the last 20
[00:40:45] years or yeah about 15 20 years sketches
[00:40:47] are becoming more common and you can use
[00:40:49] these in combination with each other.
[00:40:51] Sampling is very rare.
[00:40:53] I think only the Germans and Umbra and
[00:40:56] SQL server do use sampling. I'm not
[00:40:58] aware of any other system does sampling.
[00:41:00] Uh and then ML stuff, nobody's putting
[00:41:02] this in production yet. The research is
[00:41:04] very promising, but it's still it's very
[00:41:06] early. [snorts]
[00:41:07] >> Yes.
[00:41:13] >> The question the question is how do I
[00:41:14] deal with very high high columns? This
[00:41:18] these slides here. Yes, we'll get there.
[00:41:19] Yeah. [snorts] All right. So again,
[00:41:21] histogram is just think of like a
[00:41:22] hashmap where you keep track of a hash
[00:41:24] table keep track of like for every
[00:41:25] unique value in my column. I'm going to
[00:41:28] maintain the number of times that I've
[00:41:29] seen it. Right? So say I have I'm
[00:41:33] keeping track of people's age or
[00:41:34] something like this and it's only for
[00:41:36] people age 1 to 15, right? I just keep
[00:41:39] track of for every single unique value.
[00:41:40] Here's the here's the number number of
[00:41:42] occurrences that that they have. So in
[00:41:44] my toy example here, I'm going to I have
[00:41:47] to maintain the original column, the
[00:41:49] original data in the table itself. But
[00:41:51] now I got to maintain this other
[00:41:52] histogram that keeps track of every sing
[00:41:53] unique value I have in that column. So
[00:41:56] my example here, it's small, right? It's
[00:41:57] only 60 bytes plus the maybe 32 or 64
[00:42:00] bytes for each occurrence. But think in
[00:42:03] terms of like billions of entries in a
[00:42:06] column, right? to store one billion
[00:42:09] values with 32-bit numbers, it's going
[00:42:11] to be uh four gigs
[00:42:15] like four four something. Yeah, four
[00:42:16] point something gigs, right? So that's
[00:42:19] not the the data itself. That's the
[00:42:20] histogram about the data from one
[00:42:22] column. So for every single column I got
[00:42:25] to make if I'm going to record all the
[00:42:26] unique values and and the counts for
[00:42:28] them, then that's obviously going to be
[00:42:29] really really big and I don't want to do
[00:42:31] that.
[00:42:32] >> What's that?
[00:42:34] The question the count is small but
[00:42:36] still like could be
[00:42:40] right. Um yeah in that case if if every
[00:42:43] value is unique then the count is one.
[00:42:45] Yes and that's small.
[00:42:48] >> You can bit pack that. Yes but still
[00:42:50] like it's that's for one column in one
[00:42:52] table. If my table has 100 columns or
[00:42:55] hundreds of columns then it becomes
[00:42:57] super expensive to do. [snorts]
[00:42:59] So we gota we gota we got to reduce the
[00:43:01] size. And there's a couple tricks to do
[00:43:02] this. One is do an equidith histogram
[00:43:05] where instead of storing every unique
[00:43:07] value as a with their own separate
[00:43:09] count, I'm going to bucket them together
[00:43:12] uh
[00:43:14] where the the size of each bucket will
[00:43:16] be the same. Like the width of the
[00:43:17] bucket is going to be the same, hence
[00:43:19] the name echo width. So I would convert
[00:43:21] my original histogram into something
[00:43:22] like this where I'm just going to store
[00:43:23] now the range of values that I have and
[00:43:26] then a separate count for that, right?
[00:43:30] >> [snorts]
[00:43:31] >> So if now I want to get the estimate
[00:43:34] like say does something equal something
[00:43:36] how many times is that going to occur? I
[00:43:38] would figure out what bucket it is it's
[00:43:40] in take whatever the count is and divide
[00:43:41] it by the number of values in the bucket
[00:43:43] like size four. Again we'll cover those
[00:43:46] formulas in a second.
[00:43:48] Another approach is to use equidith
[00:43:51] histograms and these are shown to be
[00:43:52] better than equidith because they are
[00:43:56] uh
[00:43:58] because again you're for the for the the
[00:44:01] heavy hitters they're not going to get
[00:44:03] sort of lost in the noise with a bunch
[00:44:06] of low rank or low occurrence uh values.
[00:44:09] So the idea is here now I don't want the
[00:44:12] width of the bucket to be the same. I
[00:44:13] want the height of the bar per bucket to
[00:44:16] be the same. So in this case here the
[00:44:18] the height of just putting 14 and 15
[00:44:20] together the height combining them will
[00:44:22] be 12 right versus like bucket three
[00:44:25] here I'm going to put 9 10 11 12 13 and
[00:44:28] the height of that is going to be nine
[00:44:30] right so this is a way to sort of handle
[00:44:33] the outliers because now they're going
[00:44:34] to be sort of uh you have a more
[00:44:37] accurate count because there's fewer
[00:44:38] elements within each each bucket
[00:44:40] potentially.
[00:44:43] So now related to to his his problem or
[00:44:46] his question their question about like
[00:44:48] what about uh if I have like heavy
[00:44:51] hitters or
[00:44:53] values that occur a lot wouldn't be nice
[00:44:55] to have you know accurate counts of
[00:44:57] those things. So I can do what's called
[00:44:58] an Nbias histogram where I'm going to
[00:45:01] use n minus one buckets to keep track of
[00:45:04] an exact count for the most frequent
[00:45:06] keys that I see. And then for everybody
[00:45:08] else, they get thrown in this last
[00:45:10] bucket R that's just trying to say,
[00:45:12] okay, well, you know, you're probably
[00:45:13] not gonna need these keys that often.
[00:45:14] They don't occur that often. So
[00:45:16] therefore, it's okay to have a less
[00:45:17] accurate count for those guys. So you
[00:45:20] just basically look at what are the all
[00:45:22] my most frequent keys and then I store
[00:45:24] them as again exact counts, but then
[00:45:26] everybody else, sorry, everybody else
[00:45:27] just ends up with this R1 over here.
[00:45:32] >> Yes. So
[00:45:42] >> the question is if if the distrib
[00:45:48] if the distribution of the values change
[00:45:50] where now there's a new most frequent
[00:45:52] key do I have to rebuild this? Yes.
[00:45:56] And that's what the analy that analyze
[00:45:57] command I did basically goes through
[00:45:58] wipes away whatever my histograms are
[00:46:00] and recomputes them. Now post be a
[00:46:03] little bit try to be a little bit more
[00:46:05] clever and do things like uh keep track
[00:46:08] of without running analyze my explicit
[00:46:11] analyze but like without if I'm doing
[00:46:13] this sort of incrementally I can keep
[00:46:14] track of here's the pages that haven't
[00:46:16] modified since the last time I analyze
[00:46:18] them. So don't don't reanalyze them and
[00:46:20] then you have to incrementally update
[00:46:21] update these things but again that that
[00:46:24] can you can cause errors and then cause
[00:46:25] problems but whatever like it's an
[00:46:28] approximation anyway so you're already
[00:46:29] you're going to be errorrone.
[00:46:31] Yes.
[00:46:34] >> How the histogram is implemented in the
[00:46:35] case of postcript I showed you. It's
[00:46:37] just an array of values.
[00:46:40] Nothing fancy.
[00:46:45] [snorts] All right. Sketches.
[00:46:47] Again, I think uh the countman sketch
[00:46:49] we've already covered, but there's
[00:46:50] basically
[00:46:52] versions of of sketches that that
[00:46:53] provide different uh can answer
[00:46:56] different questions like what's the most
[00:46:57] frequent items? I use countman sketch.
[00:46:59] How do I count the number of distinct
[00:47:00] items? uh efficiently I use a hyper log
[00:47:02] log and there's a lot of great open
[00:47:05] source imitation now uh the Apache data
[00:47:08] sketches is probably the best one
[00:47:09] originally started out in Java but now I
[00:47:11] think there's there's there's
[00:47:12] implementations in Rust and C++ right
[00:47:14] those are those are probably the best
[00:47:16] ones but Google Zetta sketch I don't
[00:47:18] think they're actually maintaining that
[00:47:19] anymore um but it is an open source one
[00:47:22] so you can replace all your histograms
[00:47:24] in theory with sketches
[00:47:26] uh because they'll give you the same
[00:47:28] sort of they sort of answer the same
[00:47:30] kind questions and in some cases they're
[00:47:32] they're more easily mergeable than than
[00:47:34] histograms. Um but the you know they
[00:47:37] they have different different
[00:47:38] properties.
[00:47:40] All right. So I'm going to skip the
[00:47:41] countman sketch and the hyperlo log
[00:47:43] stuff. Um because I want I want to get
[00:47:46] to the um
[00:47:49] let me cover sampling real quickly. Um
[00:47:52] because I I we want to get to the
[00:47:54] cardality stuff because we need that for
[00:47:55] the next homework. All right. So
[00:47:56] sampling again is just that you have a
[00:47:58] subset of the table or the data or you
[00:47:59] you examine a subset of the table the
[00:48:01] data you want to access and try to
[00:48:03] predict the the properties of it. So you
[00:48:05] can either maintain a separate read only
[00:48:06] copy of of of your data like a small
[00:48:10] portion of it and then you do all your
[00:48:11] estimates on that thing on that on that
[00:48:13] sample or you can do a sample against
[00:48:15] the real data. Um but you got to be
[00:48:17] careful not to interfere with queries
[00:48:19] that are actually running uh for real
[00:48:21] not just sampling information. And so
[00:48:23] the way uh SQL server does it, they do
[00:48:25] it, they do the first one. I don't know
[00:48:27] of anybody does that does the second
[00:48:29] one. Uh my SQL kind of does the second
[00:48:32] one, but they're not really sampling to
[00:48:33] get stats. What my SQL will do in some
[00:48:36] cases, they will recognize you have like
[00:48:37] a subquery in the middle of the
[00:48:39] optimizer, stop what they're doing, go
[00:48:42] run that query for real, and get the
[00:48:45] result back. So it's it's not exactly
[00:48:47] sampling like but it is going as the
[00:48:50] real data. Um in SQL server's case if
[00:48:54] they when they start running the
[00:48:56] optimizer if they recognize I don't have
[00:48:57] stats uh like when the query shows up
[00:49:00] they'll stop the optimization process
[00:49:02] run analyze and go collect a sample and
[00:49:04] go collect statistics and then come back
[00:49:05] to the optimizer [snorts] right because
[00:49:07] they don't they don't make bad
[00:49:08] predictions
[00:49:10] all right so basic idea is like this
[00:49:12] like you have a table of a bunch of
[00:49:13] people you want to find people all in
[00:49:15] your database that are that are above
[00:49:17] age 50 so the sample would basically do
[00:49:20] a random uh pick random tupils from this
[00:49:23] uh and there's different ways to do uh
[00:49:25] you know more sophisticated types of
[00:49:28] sampling and then I have my little
[00:49:29] sample here I have Obama Tupac and DJ
[00:49:31] cache and I have their status what
[00:49:33] what's going on when their lives and
[00:49:34] then now what I want to estimate what
[00:49:36] the selectivity is for all the people
[00:49:38] that are age uh greater than 50 in this
[00:49:41] case here it's only Obama is that old
[00:49:43] because Tupac's dead and he's only 21
[00:49:45] you 21 or 20 how old are you 23 off by
[00:49:49] two
[00:49:51] Right. Uh so in that case here the table
[00:49:54] the the probability of people above age
[00:49:56] 50 is is one/ird and that you would say
[00:49:59] that roughly maps to what it is in the
[00:50:01] in the real table.
[00:50:03] Right. [snorts]
[00:50:06] All right. So with either samples the
[00:50:08] the sketches or the histograms we now
[00:50:11] want to predict the the cardality of the
[00:50:15] different operators. how much data or
[00:50:17] how many tupils is each operator going
[00:50:19] to produce because then we can use that
[00:50:21] to say well how as a as a as a the
[00:50:24] backbone for other decisions like all
[00:50:26] right how many tuples I'm going to
[00:50:28] produce that'll determine how much data
[00:50:29] I may have to read or write in my other
[00:50:31] operators in in my query plan [snorts]
[00:50:34] and so the the basic things we need to
[00:50:36] be able to predict is what the selection
[00:50:38] conditions could be for filters like if
[00:50:40] I have a predicate on a table how many
[00:50:43] tupils are going to match that predicate
[00:50:44] and therefore how many pupils are going
[00:50:45] to come as as as the part of the output
[00:50:47] or if I do a join and I join two tables
[00:50:50] together how many tuples are going to be
[00:50:51] produced from doing that join and then
[00:50:54] likewise if I'm doing like a distinct
[00:50:56] value estimation for like a group by
[00:50:57] clause how many tuples do I expect to
[00:50:59] come out of that so if you have those
[00:51:01] three estimations then that's the the
[00:51:03] building block you need to do more
[00:51:04] complex things like multiple joins and
[00:51:06] and group eyes and other sorting things.
[00:51:10] So the way this is going to work is that
[00:51:12] we're going to produce what are called
[00:51:13] derable statistics that we can extract
[00:51:14] from the statistical summaries the JSON
[00:51:17] has collected and and built for our from
[00:51:20] our tables. And so for this we'll say
[00:51:23] you know the the n subscript r say the
[00:51:26] number of pupils we have in the
[00:51:27] relation. And then we'll have this new
[00:51:28] function v that's going to tell us the
[00:51:30] number of distinct values we would have
[00:51:32] for an attribute a. So not saying like
[00:51:35] again this point like is it greater than
[00:51:37] a or less than a. just saying like for a
[00:51:39] given value a how many occurrences do I
[00:51:42] have of this in my in my columns and
[00:51:45] then I want to comput what's called the
[00:51:47] selection cardality is going to be the
[00:51:48] average number of tuples that I expect
[00:51:50] to produce for a given attribute a given
[00:51:54] the number of of of tupils that I have
[00:51:56] in my relation
[00:51:58] and then from this that's what we use to
[00:52:00] basically estimate the the selectivity
[00:52:02] so depending on what the predicate looks
[00:52:04] like there's different formulas we're
[00:52:05] going to use for the statistics we've
[00:52:07] collected And we use histograms as as
[00:52:09] the basic version of this to say here's
[00:52:11] here's the number of tuples I expect to
[00:52:12] qualify for my my my predicus or
[00:52:15] whatever it is the operation I'm doing
[00:52:17] and therefore that's the number of
[00:52:18] tuples I expect to come out of my of my
[00:52:21] operator.
[00:52:23] So let's look at the easiest one. Easy
[00:52:26] one's going to be a a a predicate where
[00:52:28] age equals nine an equality predicate
[00:52:31] something equals something right? So if
[00:52:33] I had my histogram I had before and I
[00:52:35] had the you know the the exact number of
[00:52:37] occurrences per for every sing unique
[00:52:39] value then compute the selectivity of
[00:52:42] this predicate it's just taking the
[00:52:44] number of occurrences divided by the the
[00:52:47] number of unique values that or unique
[00:52:50] uh number of tuples that I have in my in
[00:52:53] in my table right so now for this
[00:52:56] activity age equals 9 I just go look at
[00:52:58] my histogram what's the number of pupils
[00:52:59] I expect to match in this case four so
[00:53:02] therefore or the selectivity of this
[00:53:04] predicate is 4 / 45 and I get 0.088.
[00:53:09] So this is saying that for all the the
[00:53:11] the the tubles that are in my table
[00:53:13] people or table r here when I apply this
[00:53:16] predicate 0.08 will match
[00:53:22] but again we said that we don't want to
[00:53:23] maintain an exact count for the number
[00:53:25] of values in a histogram. We want to use
[00:53:27] one of these equidith ones for example.
[00:53:29] So now my approximation is going be
[00:53:32] based on whatever bucket I land in case
[00:53:35] here age nine lands this one 9 to 13
[00:53:37] inclusive. So now my estimation is going
[00:53:40] to be the the the count here for this
[00:53:43] bucket which is nine divided by the
[00:53:45] number of values that are within this
[00:53:47] range of the bucket five. Then now that
[00:53:50] gives me my estimate here.
[00:53:54] What's the problem with this approach?
[00:53:59] Yes.
[00:54:04] The statement is it it represents not
[00:54:06] multiple predicates but multiple values
[00:54:08] in the bucket, not just the one I'm
[00:54:10] looking for. Absolutely. Yes. I'm also
[00:54:12] assuming that like the it's a continuous
[00:54:16] range of values, right? In this example
[00:54:18] here, it's it's it's people aged 9 to
[00:54:20] 13. not a big deal but like think of
[00:54:22] like really big tables with really big
[00:54:24] you know really big numbers I may have
[00:54:27] gaps in in that sequence right
[00:54:30] so I'm assuming here that like for all
[00:54:33] the values that appear in this bucket
[00:54:35] they the they occur with the same
[00:54:38] probability as all the other values in
[00:54:40] the same bucket that's why I'm dividing
[00:54:42] by by five so the dirty secret in cost
[00:54:46] models is that there's a bunch of
[00:54:48] assumptions we have to make because we
[00:54:50] don't accurate statistics and a complete
[00:54:52] view of the data. Those are assumptions
[00:54:53] we have to make in our formulas in order
[00:54:55] to make this thing probably even
[00:54:56] tractable.
[00:54:58] And again, I keep bringing up like, oh,
[00:55:00] the enterprise systems do this, Oracle
[00:55:01] does that. Like this is what the this is
[00:55:04] what you this is one of the big
[00:55:05] difference you're going to get when you
[00:55:06] pay, you know, Oracle millions and
[00:55:08] millions of dollars and and SQL server
[00:55:10] millions of dollars for your data their
[00:55:11] data systems like their cost model is be
[00:55:14] way more sophisticated. Their form is be
[00:55:15] way more sophisticated than what
[00:55:17] Postgress has. And to their point, can't
[00:55:18] just post write a better one. Sure, but
[00:55:20] like you need people that are like
[00:55:22] experts in in query optimizers and
[00:55:24] they're not they're not cheap, right? A
[00:55:26] lot of them are the best ones at SQL
[00:55:28] Server or the Germans, right?
[00:55:32] So, one of the assumptions we're going
[00:55:33] to make is that we're going to assume
[00:55:34] that the distribution of data is be
[00:55:35] uniform that the occurrence of every
[00:55:38] single value within a column is going to
[00:55:41] appear with the equal probability of
[00:55:42] other all other columns or other values
[00:55:44] in that same column. Now, we can
[00:55:46] mitigate this a little bit by keeping
[00:55:48] track of the heavy hitters, right?
[00:55:50] Keeping track of the most frequent
[00:55:51] values separately in the NBIS histogram.
[00:55:54] But for everything else that isn't a
[00:55:55] heavy hitter, we we have to make this
[00:55:57] assumption and and it falls apart
[00:56:00] some cases.
[00:56:02] Now, we have also do a range predicate.
[00:56:04] We'll see this problem again, right? So,
[00:56:06] here I want to say give me all the
[00:56:07] people that are age greater than seven,
[00:56:09] greater than equal to seven. Well, I
[00:56:10] know that I want to include everybody
[00:56:13] all the values up up above nine and
[00:56:15] above, right? Because it's greater than
[00:56:16] equal to seven. So I know that from
[00:56:19] starting at this bucket range here over
[00:56:21] there, I want to include all of that in
[00:56:23] in my my estimation, right? So I know
[00:56:26] that the number of occurrences will be
[00:56:28] at least 9 + 12. But the challenge is
[00:56:31] going to be this middle one here where
[00:56:33] seven is in the middle between six and
[00:56:35] eight. So again, if I'm assuming uniform
[00:56:37] distribution of the the possible values,
[00:56:40] I'm going to say uh take the count of 12
[00:56:43] divide by three because there's three
[00:56:44] unique values inside this bucket and I
[00:56:46] multiply that two because I know it's at
[00:56:48] least seven or eight in this,
[00:56:52] right? So I get my estimation like this.
[00:56:56] But again, as I said, not only are we
[00:56:57] assuming that the values are uniform,
[00:56:58] we're also assuming the values are
[00:57:00] continuous. For my small range here,
[00:57:02] it's not a big deal, but think of really
[00:57:03] large numbers. You could have gaps and
[00:57:06] you may be counting tupils that appear
[00:57:07] that don't actually exist, but you don't
[00:57:09] know because you're trying you're based
[00:57:11] on approximations.
[00:57:12] [snorts]
[00:57:14] Negation is is is pretty easy, right? A
[00:57:18] not equals, it's just the opposite, the
[00:57:20] inverse of of equals. So all we have to
[00:57:22] do is say, all right, for age doesn't
[00:57:24] equal two, well, I figure out what is
[00:57:26] the selectivity of age equals to, which
[00:57:29] I would land in this bucket here. do the
[00:57:30] same formulas that we had before and
[00:57:32] then whatever that is that's what I I
[00:57:34] take one minus that because it's it's I
[00:57:37] know that if it's equal this it should
[00:57:39] be in here it's not equal to that then
[00:57:41] it's all these other ones here
[00:57:45] so the key observation and maybe already
[00:57:47] slipped up and said this already is that
[00:57:50] we are at the end of the day the
[00:57:52] selectivity estimates they're just
[00:57:54] probabilities
[00:57:55] right what's the probability that a that
[00:57:57] that a pupil will match within whatever
[00:58:00] data I'm looking at according to my
[00:58:02] predicate and based on what my predicate
[00:58:04] is I can go apply these different
[00:58:05] formulas.
[00:58:07] So okay well this is if it's a single
[00:58:10] predicate like something equal something
[00:58:12] something you know not equal to
[00:58:13] something then that's pretty
[00:58:14] straightforward but now if I have
[00:58:17] multiple predicates then it gets more
[00:58:19] tricky and now you come back to like
[00:58:21] stats 101 and
[00:58:24] you can just make other assumptions to
[00:58:26] to simplify the problem which again at
[00:58:29] the cost of accuracy. So in this case
[00:58:31] here I have two predicates age equals 2
[00:58:33] and name like a wild card right and so
[00:58:37] for each of those separate predicates I
[00:58:39] can calculate different selectivities
[00:58:40] for them right but now how do I combine
[00:58:43] them right if you go back to like a
[00:58:46] simple ven diagram you sort of think of
[00:58:48] like this the first pink circle is the
[00:58:50] first predicate and the blue circle is
[00:58:52] the second predicate but what I really
[00:58:53] want to find is this middle overlapping
[00:58:55] part here so how can I do that in
[00:58:58] statistics
[00:59:02] If you assume they're independent
[00:59:05] uh the two independent predicates then
[00:59:07] you can just multiply them together and
[00:59:09] produce the output or get the
[00:59:11] selectivity that way. So the second big
[00:59:13] assumption all these cost models are
[00:59:14] going to make is that your predicates
[00:59:15] are independent,
[00:59:17] right? And that assuming it's all
[00:59:19] conjunction clauses that you take the
[00:59:21] first probability or the first
[00:59:22] selectivity computed for the first
[00:59:23] predicate and you can multiply it by by
[00:59:25] another predicate
[00:59:29] for uh again for for disjunction is a
[00:59:33] bit more complicated but you basically
[00:59:34] you know the math sort of works out like
[00:59:36] this right. The challenge of course is
[00:59:39] going to be that oftentimes or in many
[00:59:42] cases these aren't independent,
[00:59:45] right? And and your your estimates can
[00:59:47] get way off. And so there's a bunch of
[00:59:49] little statistical tricks you can do to
[00:59:50] try to mitigate this problem because
[00:59:52] you're going to end up you're going to
[00:59:52] end up underestimating what the true
[00:59:54] cardality or reflectivity is going to
[00:59:55] be. They won't talk about this too much
[00:59:57] but in the case of SQL server what they
[00:59:59] did started doing in 2014 they have a in
[01:00:02] the book they talk about this is that
[01:00:04] when I start having a lot of predicates
[01:00:06] that are all and together in conjunction
[01:00:08] clauses I rank them based on their
[01:00:11] selectivity. So the ones that are most
[01:00:12] selective independently are the ones I
[01:00:14] put in sort of the front of the of the
[01:00:16] so the clause clause. But then all
[01:00:18] subsequent predicates I'm going to
[01:00:20] diminish their their their their
[01:00:24] probability by some waiting factor that
[01:00:26] that exponentially goes down so that I
[01:00:30] end up predicting I don't I end up
[01:00:32] underpredicting less because if I
[01:00:34] multiply them together then I can get to
[01:00:36] really small predictions that are way
[01:00:37] off.
[01:00:39] Again, we we'll cover that more in the
[01:00:41] advanced class, right? So, uh
[01:00:44] disjunction again for ores just trying
[01:00:46] to find this whole thing and you just
[01:00:47] assume that they're independent and like
[01:00:49] this. All right. So, let me show the
[01:00:50] problem of of correlated attributes and
[01:00:52] you'll see why the Microsoft trick sort
[01:00:54] of works. So, this is a really simple
[01:00:55] example from uh a famous IBM researcher
[01:00:58] from a few years ago. Save a simple
[01:01:00] database that has two tables or has you
[01:01:04] makes and models, right, for cars. And
[01:01:06] then [snorts] a query shows up where I
[01:01:07] say find me all the records where make
[01:01:09] equals Honda and model equals accord.
[01:01:12] Right? So if I apply the two assumptions
[01:01:15] I've talked about so far, the
[01:01:16] independence assumption and the
[01:01:17] uniformity assumption, then if I plug
[01:01:20] and chug my formulas, I end up with a
[01:01:22] with a selectivity like this. So again,
[01:01:25] I have 10 makes like Honda, Toyota, and
[01:01:27] Ford, right? So if I'm trying to find
[01:01:29] where make equals one, you know, Honda,
[01:01:31] one match. So that's be one over 10. So
[01:01:34] one out of 10 tup two bowls will match.
[01:01:36] If I'm trying to find an accord, you
[01:01:38] know, say I have 100 cards and one of
[01:01:40] them is a chord, then that's be one over
[01:01:41] 100. Now if I assume those two
[01:01:43] probabilities are independent, I multip
[01:01:46] multiply the two together and I get
[01:01:47] 0.001.
[01:01:50] But we know that's incorrect as humans
[01:01:51] because who makes an accord?
[01:01:54] Only Honda, right? So the correct
[01:01:57] selectivity is really one over 100,
[01:02:01] right? Okay, we don't need that one over
[01:02:02] 10 part to multiply that. So, all right.
[01:02:04] So, you see there's there's small
[01:02:05] numbers. You know, you say, "All right,
[01:02:06] that's not not that far off. Who cares?"
[01:02:08] But you're actually order magnitude off.
[01:02:11] And so, again, think of think of huge
[01:02:13] tables, right? I went from thinking
[01:02:15] there's, you know, 100 million tupils uh
[01:02:18] or yeah, when it's really a billion
[01:02:20] tupils,
[01:02:22] right? That's huge and that that that's
[01:02:24] going to matter.
[01:02:26] So again, this is why the Microsoft
[01:02:27] trick what they try to do is
[01:02:30] uh if you they'll say, "Well, I'll I'll
[01:02:33] do the one over 100 first and then I'll
[01:02:36] still do the one over 10, but I'll put a
[01:02:38] little weight factor on it so that it
[01:02:39] it's not as pronounced in in my final
[01:02:42] calculation." So they're never going to
[01:02:43] be exactly 0.001,
[01:02:45] but it won't be as bad as being an order
[01:02:48] of magnitude off.
[01:02:50] Again, it they it's a it's a trick that
[01:02:54] works for them. I don't think everybody
[01:02:55] else any else does it. I don't know
[01:02:56] whether I don't think there's any
[01:02:58] statistical guarantees you can
[01:02:59] extrapolate from it.
[01:03:02] All right, the last one I'll talk about
[01:03:03] is how to join size estimation. And this
[01:03:05] is where can this is where it all falls
[01:03:07] apart. Um
[01:03:09] [snorts] so the problem I'm trying to
[01:03:11] solve is I got two two relations R and
[01:03:13] S. And I want to say what's the range of
[01:03:16] possible sizes of the number of tupils I
[01:03:18] expect to come out of doing that join.
[01:03:22] Right? So how many tuples are going to
[01:03:25] match based on my join predicate. So
[01:03:27] think about all the things I said that
[01:03:28] are hard to do just dealing with one
[01:03:31] table trying to figure out what you know
[01:03:33] you know for a predicate on it. How many
[01:03:35] tables that come out of that? Now I got
[01:03:36] to say I got two tables and how what's
[01:03:39] the con what's the the likelihood that
[01:03:41] the values in in one table is going to
[01:03:44] match when I do my join and the values
[01:03:45] in the other table
[01:03:47] and
[01:03:50] you know if it's if it's all primary key
[01:03:52] lookups that that you can maybe figure
[01:03:54] it out be okay but if it's like primary
[01:03:56] key foreign key where it's like one
[01:03:58] tupal in the outer table will match
[01:03:59] multiple tupils in the inner table
[01:04:01] things get really bad or things get bad
[01:04:03] things get really bad when it's like
[01:04:05] It's N to N or M to M. Right? I have
[01:04:08] multiple tools in in the outer table
[01:04:09] match to multiple tools in the inner
[01:04:11] table.
[01:04:12] You're screwed. It's all over. Right?
[01:04:14] [snorts] So, one of the assumptions that
[01:04:16] we're going to make this all work is
[01:04:17] that we rely on what's called the
[01:04:20] containment principle. And this is
[01:04:22] saying that we expect that there will
[01:04:25] always be a match in in the the and for
[01:04:29] the inner table key with the outer table
[01:04:31] key. If you have to start figuring out
[01:04:32] that there there aren't going to be
[01:04:33] matches then then it just the estimates
[01:04:37] get like completely screwed up and
[01:04:39] wrong. Right? Again we're just
[01:04:40] simplifying the problem in by
[01:04:42] sacrificing uh you know the accuracy of
[01:04:45] our estimates in exchange for making it
[01:04:47] actually more trackable that we have
[01:04:49] statistics that can figure this out.
[01:04:52] [snorts] So the formula looks like this
[01:04:53] right? You basically say, you know,
[01:04:55] what's the what's the estimated size of
[01:04:57] the number of tuples I'm going to match
[01:04:59] from the the the outer table with the
[01:05:01] inner table? So R to S and likewise
[01:05:03] going the other way, what's the number
[01:05:05] the tubas are going to match from S
[01:05:06] going into R. Right? So I had that
[01:05:09] basically these two formulas there. So
[01:05:10] the cardality estimation for the join is
[01:05:13] going to be the basically the size of
[01:05:15] the cartisian product of the tables in
[01:05:18] the tupils in R and tupils in S divided
[01:05:20] by with the max selectivity of either of
[01:05:24] these two two choices like the size of
[01:05:26] the tupils are going to match in R from
[01:05:27] R to S the size of the tub is going to
[01:05:29] match from S to R in the back. Yes.
[01:05:42] Yeah. So statement is I said that
[01:05:44] [snorts] you could have a join that does
[01:05:46] match M tupils in R to M tupils in S.
[01:05:48] How does the containment principle
[01:05:49] handle that? It doesn't
[01:05:56] question what do you do if that's if it
[01:05:57] happens?
[01:06:02] The benefit of the containment
[01:06:03] principle, it allows us to
[01:06:06] use a simplified uh formula to estimate
[01:06:09] the the the number of tools I expect to
[01:06:12] come out of the join, right? Without it,
[01:06:15] if I had to consider the case, I don't
[01:06:16] like we're not talking about left out of
[01:06:17] joints, but like like I don't have
[01:06:19] matches, then like the formula doesn't
[01:06:21] doesn't work.
[01:06:23] [snorts]
[01:06:24] again like these are all in the textbook
[01:06:26] when I'm describing like and this is
[01:06:27] essentially what Postgress does that
[01:06:29] there's
[01:06:32] again it's it's uh
[01:06:35] it's not good but like but like there's
[01:06:38] nothing else right other than you know
[01:06:40] running running on samples would solve
[01:06:41] all this because you just you do the
[01:06:43] join of the sample right but if you're
[01:06:45] based on histograms or even sketches
[01:06:47] this is essentially what what you have
[01:06:49] right [snorts]
[01:06:50] all right let's walk through an example
[01:06:52] uh and see again I'm not trying to be
[01:06:54] like super depressed or dow about all
[01:06:55] this, but let me just show you like why
[01:06:58] this is super hard and why everything
[01:07:01] why it's it's like why things get really
[01:07:03] bad. We're basically going to show how
[01:07:05] we're going to propagate errors from our
[01:07:07] our bad estimates of the the bottom to
[01:07:09] our bad estimates in the middle and then
[01:07:11] produce bad estimates at the top just
[01:07:12] gets gets multiplicative. So the very
[01:07:15] first thing we do say this is the query
[01:07:16] plan we want to uh run and again for
[01:07:18] this one we don't have to have the
[01:07:19] physical operators we can just do this
[01:07:21] at the logical level to try to predict
[01:07:22] how many tools to expect to come out of
[01:07:24] each different operators. So for A and A
[01:07:28] and C there's no filter on it. But in
[01:07:30] the case of B uh we have this B is
[01:07:33] greater than 100. Um which actually I'm
[01:07:35] missing. I cheated in name space. I
[01:07:38] didn't show a filter operator but assume
[01:07:40] that's the the B greater than B ID
[01:07:42] greater than 100 at the bottom. Right?
[01:07:43] So for each of these I can compute the
[01:07:45] the selectivity of them. And for B I
[01:07:47] greater than 100 assume that I have
[01:07:49] again some histogram that allows me to
[01:07:50] do that that approximation. And then now
[01:07:53] I want to go at the next level in the
[01:07:54] query plan. what's the cardality of the
[01:07:58] of the join operator right the the first
[01:08:00] one here so for this I use that same
[01:08:03] formula from the last slide I say okay I
[01:08:05] have these number two coming up from A
[01:08:07] and B and then what's the selectivity I
[01:08:10] expect for matching a ID to B ID or the
[01:08:14] selectivity of matching B I greater than
[01:08:15] 100 and then take the max of that and
[01:08:17] that's tell me going to be sort of the
[01:08:18] upper bound the number of tupils I
[01:08:20] expect to potentially come out of uh
[01:08:23] this you know this joint operator and
[01:08:25] this again why we're doing the
[01:08:26] containment principle is that if I have
[01:08:28] to consider that I'm not going to have a
[01:08:29] match uh you know or doing a left outer
[01:08:32] joint where I'm producing more tables
[01:08:33] than I expect because I don't have a
[01:08:35] match then this this doesn't work but if
[01:08:37] we we simplify it it does work at least
[01:08:40] reasonably but then say now then I take
[01:08:43] I got to take the the output of that
[01:08:45] first join and now that's going to be
[01:08:47] fed as input to the next join right on
[01:08:50] the on the on the left hand side because
[01:08:53] I need to know how many I to come in
[01:08:55] plus now the scan I'm doing on on on C.
[01:09:00] So now what we see is that we're we're
[01:09:01] we're starting with sort of basic
[01:09:03] estimations for the for the leaves of
[01:09:04] the cord plan, right? That one we might
[01:09:06] be okay with. But then when we start
[01:09:08] doing the join, we're taking the output
[01:09:09] of that join feeding that as the input
[01:09:11] to the next join. And so if our formulas
[01:09:13] and our estimates and our histograms are
[01:09:14] all off for the lower parts, then that
[01:09:16] just makes this even even worse going
[01:09:18] up,
[01:09:20] right? Right. And so basically we have
[01:09:21] bad bad estimates here and then bad
[01:09:24] estimates here and and then for three
[01:09:26] tables not a big deal but think of like
[01:09:28] the 1500 table joint I mentioned before
[01:09:30] it's it's all house of cards going going
[01:09:32] to the top and the research literature
[01:09:34] shows that the all the data systems
[01:09:38] because of the these different
[01:09:39] assumptions I'm talking about they end
[01:09:41] up underestimating the number of tupils
[01:09:43] they expect at different operators and
[01:09:45] as you add more joints the
[01:09:46] underestimations get higher and higher.
[01:09:50] And so, okay, so what's the problem with
[01:09:51] that? Well, now I I think I the size of
[01:09:54] the data I'm coming up to me at some
[01:09:55] higher point in the tree is low. So
[01:09:57] therefore, I don't want to do hash join
[01:09:59] because I'm only expecting 10 tupils.
[01:10:01] And so I I choose a nested loop join
[01:10:02] because that's really efficient if
[01:10:03] everything fits in memory. But then if
[01:10:05] I'm underestimating, I might be getting,
[01:10:07] you know, a million tupils and I really
[01:10:08] wanted a hash join. And now I'm doing
[01:10:10] the, you know, the slow nested loop
[01:10:12] join.
[01:10:16] >> The question is, why can't you switch on
[01:10:17] the spot?
[01:10:20] >> [snorts]
[01:10:20] >> advanced class. So his question so what
[01:10:22] he's asking is um it's called adaptive
[01:10:25] join uh sorry adaptive query processing
[01:10:28] a AQP and basically what that is you put
[01:10:31] little hooks in the query plan to say
[01:10:34] when I generate when I when the
[01:10:35] optimizer gen generated this plan I
[01:10:38] expect the data to look like this I
[01:10:39] expect to get 10 tupless coming up
[01:10:40] therefore I want to do next loop join if
[01:10:42] I see if I see more than 10 or more than
[01:10:44] some threshold stop what I'm doing throw
[01:10:46] away the work and switch now to a hash
[01:10:48] join
[01:10:50] Right. So, uh
[01:10:53] SQL server can do something like this.
[01:10:56] Uh
[01:10:58] uh Snowflake can do something like this
[01:11:00] as well, but they they for for group
[01:11:02] eyes and things like that, right? That's
[01:11:04] hard to do. Only the the high-end
[01:11:07] systems do it and it's very rare.
[01:11:09] Another the easier way to do this would
[01:11:11] be [snorts] if I I have these little
[01:11:15] hooks and say, "Okay, my estimates are
[01:11:16] just way off."
[01:11:18] rather than try to be more fine grain in
[01:11:20] the adaptation that you're like you're
[01:11:21] talking about like change the hash this
[01:11:23] change the join algorithm you say well
[01:11:24] this is all crap give up stop the query
[01:11:27] throw away all the results go back to
[01:11:29] the optimizer and say hey look you
[01:11:30] you're way off here's what I'm really
[01:11:32] seeing and then let the optimizer run
[01:11:33] again and turn a new query plan and the
[01:11:35] thought process there is like all right
[01:11:37] well my query plan is so terrible it's
[01:11:38] going to take an hour to run this so
[01:11:40] it's worth me to go back to the
[01:11:42] optimizer spend another I don't know
[01:11:44] couple you know 20 seconds or something
[01:11:46] get a better query plan because I may
[01:11:47] cut down the computation time in half.
[01:11:50] Of course, you obviously don't want to
[01:11:51] throw away what you've done if you're
[01:11:52] like almost near the end. Like it's that
[01:11:54] that's hard to do.
[01:11:56] Uh nobody does that one, I don't think.
[01:12:02] >> It's in the Yeah. So that's called
[01:12:04] adaptive query optimization. That's the
[01:12:06] advanced class. This one that's
[01:12:07] basically you can you can you sort of
[01:12:10] figure out like the the the pain
[01:12:12] tolerance of or the the for how off
[01:12:15] you're allowed to be in your estimates
[01:12:17] and you annotate the query plan so that
[01:12:18] if you see you're way off you basically
[01:12:20] abort it and go back but you you want to
[01:12:23] do this at the bottom not not the top
[01:12:25] like if you if you've been running for
[01:12:26] the queries is going to take an hour
[01:12:27] you've been running for 59 minutes you
[01:12:29] don't want to give up and go back
[01:12:32] >> dynamic across the
[01:12:36] to to that point yes the threshold I'm
[01:12:37] talking about should be dynamic based on
[01:12:39] where you are like based on uh the
[01:12:42] height you are in the query plan what
[01:12:44] the operator is uh what the data might
[01:12:47] look like how many like how many uh the
[01:12:50] size of the data at least I expect to
[01:12:51] see versus what I'm seeing right if it's
[01:12:53] you know you're reading 100 megabyte
[01:12:54] table who cares but like you know if
[01:12:56] you're in the pabyte range then yeah it
[01:12:58] matters yes
[01:13:03] >> say it again say it again sorry
[01:13:07] Yes,
[01:13:12] >> the question is um I mentioned that um I
[01:13:15] don't I don't think I have slides here.
[01:13:16] Um I mentioned that uh
[01:13:21] most data systems underestimate in
[01:13:23] joins. What happens when you under when
[01:13:24] you overestimate? [snorts]
[01:13:26] So you may end up choosing a hash join
[01:13:28] when a nest loop join would have been
[01:13:29] faster. you may end up allocating your
[01:13:32] hash table size if you're doing hash
[01:13:33] join to be larger than it actually needs
[01:13:34] to be. Of course, the opposite if you
[01:13:36] underestimate if you if you if you
[01:13:38] underestimate the size and you end up
[01:13:40] getting too big, you got to stop what
[01:13:41] you're doing and copy the hash table
[01:13:42] over. That's what postress has that
[01:13:44] problem. Um so like you end up maybe
[01:13:47] allocating more things than you actually
[01:13:49] need and you still could produce a bad
[01:13:51] query plan.
[01:13:53] All right, we have a few minutes left.
[01:13:54] So actually let me see if I have the
[01:13:58] slide here I can share.
[01:14:04] Oh, I might. Yes.
[01:14:11] Yes. Perfect.
[01:14:15] So, this is a paper um there's a there's
[01:14:18] a newer version of this paper uh
[01:14:23] that came out this year because I think
[01:14:24] it won it won best or test of time
[01:14:26] award. Um so this is from the uh the
[01:14:31] advanced topics class I taught last
[01:14:32] semester but then I almost died and I
[01:14:34] couldn't teach the rest but it was on
[01:14:36] optimization right so this is a paper
[01:14:37] from the Germans from 2015 that
[01:14:40] basically was trying to measure how good
[01:14:41] the data system different systems are
[01:14:43] for doing cost model estimations um and
[01:14:47] so for this what they basically is they
[01:14:48] use the it's called the joiner benchmark
[01:14:51] it's the IMDb database it's it's not
[01:14:53] that big but they have a bunch of
[01:14:54] queries that do a ton of joins in that
[01:14:56] on
[01:14:57] and they try to estimate how good the
[01:14:59] query model estimations are for the or
[01:15:02] the cost model estimations are for the
[01:15:03] cardality of the joints. I mean how many
[01:15:05] I expect to come out of each join as you
[01:15:07] add more joins in in your query,
[01:15:09] [snorts] right?
[01:15:11] So uh so this graph is from the from the
[01:15:13] paper from the Germans and so there's
[01:15:15] there's each of these runs going to be
[01:15:17] there's five different systems here,
[01:15:19] right? And a way I like how pulling one
[01:15:22] cord kills the camera, too. Oh, it
[01:15:23] didn't like me. All right. Uh all right.
[01:15:25] So again the the there's five different
[01:15:27] systems here along the x the x axis here
[01:15:30] it's the number of joins they have right
[01:15:32] so that zero means it's a table on a
[01:15:34] single scan on a table by itself right
[01:15:36] uh and then you add more bone joints and
[01:15:39] so what you see here are the error
[01:15:41] bounds the line going across the middle
[01:15:43] that's where the that's the exact
[01:15:46] estimate or sorry the exact measurement
[01:15:48] of the cardality of the joint operators.
[01:15:51] So if you're above the red bar, then
[01:15:52] you're overestimating. If you're below
[01:15:54] it, then you're underestimating. And you
[01:15:55] can see the the the error bounds for
[01:15:58] this, right? And so across all of them,
[01:15:59] as what you see is that they're all
[01:16:01] underestimating as you add more joins
[01:16:03] because of all those assumptions I
[01:16:04] talked about before where they're saying
[01:16:06] I expect I'm getting I'm to get fewer
[01:16:08] tupils out of my joint operator than
[01:16:10] than I than I than I actually will. And
[01:16:13] then now you're taking that
[01:16:14] underestimation, feeding that into
[01:16:15] another joint operator that then
[01:16:16] underestimates even further, right? And
[01:16:19] that's the problem gets gets compounded
[01:16:20] for this, right?
[01:16:23] >> What's that?
[01:16:27] >> Uh this this one here.
[01:16:30] Yeah. Like the question is which one
[01:16:32] this is? Right. This one's
[01:16:32] overestimating. Uh
[01:16:35] actually I I seen the slides. I know
[01:16:37] what it's coming up. Um and then there's
[01:16:39] this one here that's like way way off.
[01:16:41] Okay. So let's play a game. Who Let's
[01:16:43] take a guess what the systems are.
[01:16:46] [snorts]
[01:16:48] All right, I'll tell you the choices are
[01:16:50] uh Postgress, SQL Server,
[01:16:54] uh the German system, the fir first
[01:16:55] German system and Oracle.
[01:17:01] >> Uh what did I say? SQL server, Oracle,
[01:17:03] Postgress,
[01:17:05] uh the German system, maybe my SQL might
[01:17:07] be one of them as well. All right, so
[01:17:10] this one clearly is the is the worst.
[01:17:13] >> What do we think this is?
[01:17:15] >> Wait, I raise your hand. Okay, Postgress
[01:17:17] raise your hand to say Oracle by SQL
[01:17:21] the Germans SQL server. Okay. Um but the
[01:17:26] first one is Postgress. Last one is
[01:17:28] Hyper. Ah it was DB2 was the other one.
[01:17:30] The worst one here is Oracle,
[01:17:33] right? Even though I made a big deal
[01:17:34] about how bad postcrist is compared to
[01:17:36] Oracle one. Like I mean this was over 10
[01:17:39] years ago. So I think they fixed it
[01:17:40] since then. Uh but in this case here,
[01:17:44] SQL server is the only one doing
[01:17:45] sampling,
[01:17:47] right? And that's why their estimates
[01:17:49] are are better than the than than than
[01:17:52] the other ones, right? At least that
[01:17:53] attributes to that. Um in the case of
[01:17:58] the Germans, after this paper came out,
[01:18:00] I think in their new system, they they
[01:18:02] then added sampling and it helps a
[01:18:04] little bit, but it's not it's not that
[01:18:06] great. But again like you know DB2 and
[01:18:08] Oracle these are systems that people
[01:18:10] working on for decades and spent
[01:18:13] millions of dollars of people you know
[01:18:15] time trying to optimize these things and
[01:18:17] they still get a way off.
[01:18:20] >> This is 2015. There's a newer one that
[01:18:22] came out I think this year. Uh and I I I
[01:18:27] think the Germans win again. Um okay.
[01:18:33] >> You actually don't know because it's
[01:18:34] closed source. You don't know. They
[01:18:36] might have fixed it.
[01:18:42] >> I mean the question is like what do they
[01:18:43] what do they test it against? Uh I mean
[01:18:46] they they the Germans designed this
[01:18:47] benchmark for just this paper. So it's
[01:18:49] not like they may have gone back now
[01:18:51] since then and and improved it further,
[01:18:53] right?
[01:18:54] >> No no well there's new German system and
[01:18:56] a new German paper um that follows up on
[01:18:59] it. Okay. So this is just repeating what
[01:19:01] I already said. So
[01:19:04] right. Yeah. So just to finish up the
[01:19:07] hope the main take away this is that
[01:19:09] again we're making a bunch of
[01:19:10] assumptions of what the data looks like
[01:19:12] and what the formulas are just to make
[01:19:13] this thing the problem actually work.
[01:19:15] But in practice again for real data uh
[01:19:18] especially data with a lot of turn a lot
[01:19:20] when it's changing all the time. It it
[01:19:22] um it it can get real bad real quickly.
[01:19:25] Right. And in the end of the day, what
[01:19:26] the cost model is trying to do is
[01:19:27] estimate the number of tools we expect
[01:19:28] an operator to spit out and we use that
[01:19:30] to then to calculate how much work I
[01:19:32] have to do or where how much IO will
[01:19:34] have to do reading the data or writing
[01:19:35] the data. As I said, this is all
[01:19:37] problematic a lot of problems. [snorts]
[01:19:39] I was trying to teach as I said a the
[01:19:42] special topics class last semester of
[01:19:44] the spring all on optimization. We got
[01:19:46] up the cardality before I whatever I
[01:19:48] almost died. Uh and so there's not
[01:19:51] there's no lectures after 14. Um but if
[01:19:53] you like this kind of stuff you can
[01:19:55] check out the material here or check out
[01:19:56] that book from Microsoft that thing is
[01:19:58] phenomenal and it explains how to do the
[01:20:00] the top down query optimizer. [snorts]
[01:20:02] Okay. All right. So next class we will
[01:20:05] then that's for query optimization. Now
[01:20:07] we're talk about transactions. So this
[01:20:08] will be the second hardest part of uh of
[01:20:12] a database system and oftentimes
[01:20:14] students mind starts to bend whatever
[01:20:16] because it's it's it's be a different
[01:20:18] notion of correctness when we start
[01:20:21] doing parallel things that maybe what
[01:20:22] you're what you're used to like not
[01:20:24] memory barriers in the same way in like
[01:20:26] a low level systems class. Now it's
[01:20:28] going to have different notions of
[01:20:29] correctness and that don't seem to you
[01:20:32] know that aren't always going to be
[01:20:34] intuitive right away. Okay.
[01:20:36] All right. Hit it [music]
[01:20:40] flips acrobats
[01:20:47] over [music]
[01:20:52] [music]
[01:20:58] [music]
[01:21:00] the fortune maintain flow
[01:21:05] with the brain.
[01:21:06] >> [music]
[01:21:07] >> for Maintain flow with
[01:21:11] the grace.
[01:21:15] >> [music]
